{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXOApyaxjEdg"
   },
   "source": [
    "# Coding a Multi-Layer Perceptron (MLP) model from scratch #\n",
    "\n",
    "Having played a bit with deep-learning frameworks like Tensorflow, Keras and Pytorch, I decided to code a **Multi-Layer Perceptron** model *from scratch.*\n",
    "To keep everything simple, I'll assume a single hidden layer (which can be generalized later to have as many layers as we want) as described in the following section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqxjb9uJ1Lii"
   },
   "source": [
    "# Model Description\n",
    "\n",
    "The model that I'll implement is described in the following diagrams:\n",
    "(source : https://documents.epfl.ch/users/f/fl/fleuret/www/dlc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbN64yPN2LAa"
   },
   "source": [
    "## MLP Model - Forward Pass\n",
    "\n",
    "![MLP forward pass](images/mlp_f.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_3UV0Y-2Peh"
   },
   "source": [
    "## MLP Model - Backward Pass\n",
    "\n",
    "![MLP Backward pass](images/mlp_b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SUoNFkx1u_T"
   },
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAdcL_Dc2gv_"
   },
   "source": [
    "## Libraries & Imports\n",
    "\n",
    "I'll use the [Pytorch](http://pytorch.org/) Tensor API (quite similar to NumPy) to implement it. I won't use any high-level torch.nn models but rather implement feed-forward and back-propagation algorithm myself. I could have used numpy to do it as well but I just want to use this opportunity to get more comfortable with low-level pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9gOs8ZXBX1Wi"
   },
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TExUugMH19i3"
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "The MLP Model will include following key methods:\n",
    "\n",
    "* **init** : initializes model paramaters\n",
    "* **forward** : conducts forward pass given feature vector\n",
    "* **backward** : conducts backprop and generates the gradients\n",
    "* **optimizsation step**: Updates the model parameters using the gradients and given learning rate\n",
    "* **train**: Higher-level function to train the model using multiple epochs of \n",
    "  * Forward Pass\n",
    "  * Backward Pass\n",
    "  * Updating Model Parameters\n",
    "* **evaluate** : Evaluates trained model using a test set\n",
    "* **helper functions** : like activation, gradient of activation, loss, etc.\n",
    "\n",
    "I have implemnted all of the above using a single Python class as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wkNKEug7X2lx"
   },
   "outputs": [],
   "source": [
    "# Multi-layer Perceptron Model as a class (with one hidden layer)\n",
    "# Will try to generelize later with multiple hidden layers.\n",
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=20):\n",
    "        # Initializing paramerters, gradients & intermediate variables \n",
    "        self.params, self.grads, self.intermediate = dict(), dict(), dict()\n",
    "        self.params['w1'] = torch.normal(torch.zeros(input_dim, hidden_dim), torch.zeros(input_dim, hidden_dim).fill_(1e-6))\n",
    "        self.params['b1'] = torch.normal(torch.zeros(1, hidden_dim), torch.zeros(1, hidden_dim).fill_(1e-6))\n",
    "        self.params['w2'] = torch.normal(torch.zeros(hidden_dim, output_dim), torch.zeros(hidden_dim, output_dim).fill_(1e-6))\n",
    "        self.params['b2'] = torch.normal(torch.zeros(1, output_dim), torch.zeros(1, output_dim).fill_(1e-6))\n",
    "        self.grads_zero_()\n",
    "  \n",
    "    # Choosing tanh as activation function\n",
    "    @staticmethod\n",
    "    def sigma(x): return torch.tanh(x)\n",
    "  \n",
    "    # Derivative of tanh (x) = 1 - (tanh(x)) ^ 2\n",
    "    @staticmethod\n",
    "    def dsigma(x): return 1 - torch.tanh(x).pow(2)\n",
    "  \n",
    "    # Choosing MSE as loss function which works well in this case. Will explore\n",
    "    # cross_entropy later\n",
    "    @staticmethod\n",
    "    def loss(v, t): return (v - t).pow(2).mean()\n",
    "  \n",
    "    # Derivative of MSE funcion\n",
    "    @staticmethod\n",
    "    def dloss(v, t): return 2 * (v - t)\n",
    "  \n",
    "  \n",
    "    # Setting all gradients to zero (we'll do so before backward pass)\n",
    "    def grads_zero_(self):\n",
    "        self.grads['dl_dw1'] = torch.zeros_like(self.params['w1'])\n",
    "        self.grads['dl_db1'] = torch.zeros_like(self.params['b1'])\n",
    "        self.grads['dl_dw2'] = torch.zeros_like(self.params['w2'])\n",
    "        self.grads['dl_db2'] = torch.zeros_like(self.params['b2'])\n",
    "\n",
    "    # Forward Pass : Using the above diagram as reference\n",
    "    def forward(self, x):\n",
    "    \n",
    "        # Model params\n",
    "        w1 = self.params['w1']\n",
    "        w2 = self.params['w2']\n",
    "        b1 = self.params['b1']\n",
    "        b2 = self.params['b2']\n",
    "    \n",
    "        # Calculating intermediate values and final value\n",
    "        s1 = torch.matmul(x, w1) + b1\n",
    "        x1 = self.sigma(s1)\n",
    "        s2 = torch.matmul(x1, w2) + b2\n",
    "        x2 = self.sigma(s2)\n",
    "    \n",
    "        # Caching intermediate values for backward pass\n",
    "        self.intermediate['x']  = x\n",
    "        self.intermediate['s1'] = s1\n",
    "        self.intermediate['x1'] = x1\n",
    "        self.intermediate['s2'] = s2\n",
    "        self.intermediate['x2'] = x2\n",
    "    \n",
    "        # Return result calculated above\n",
    "        return self.intermediate['x2']\n",
    "  \n",
    "    # Backward Pass : Using the above diagram and equations as reference\n",
    "    def backward(self, t):\n",
    "    \n",
    "        # restoring cached values\n",
    "        dl_dw1 = self.grads['dl_dw1']\n",
    "        dl_dw2 = self.grads['dl_dw2']\n",
    "        dl_db1 = self.grads['dl_db1']\n",
    "        dl_db2 = self.grads['dl_db2']\n",
    "        w2     = self.params['w2']\n",
    "        x  = self.intermediate['x']\n",
    "        s1 = self.intermediate['s1']\n",
    "        x1 = self.intermediate['x1']\n",
    "        s2 = self.intermediate['s2']\n",
    "        x2 = self.intermediate['x2']\n",
    "    \n",
    "        #Calculate gradients\n",
    "        dl_dx2 = self.dloss(x2, t)\n",
    "        dl_ds2 = dl_dx2 * self.dsigma(s2)\n",
    "        dl_dw2 += torch.mm(x1.t(), dl_ds2)\n",
    "        dl_db2 += dl_ds2.sum(dim=0)\n",
    "        dl_dx1 = torch.mm(dl_ds2, w2.t())\n",
    "        dl_ds1 = dl_dx1 * self.dsigma(s1)\n",
    "        dl_dw1 += torch.mm(x.t(), dl_ds1)\n",
    "        dl_db1 += dl_ds1.sum(dim=0)\n",
    "    \n",
    "    # Function to update model params\n",
    "    def optimize_step(self, lr=0.0001):\n",
    "        self.params['w1'] -= lr * self.grads['dl_dw1']\n",
    "        self.params['w2'] -= lr * self.grads['dl_dw2']\n",
    "        self.params['b1'] -= lr * self.grads['dl_db1']\n",
    "        self.params['b2'] -= lr * self.grads['dl_db2']\n",
    "      \n",
    "    # Function to train the model using a minibatch training set loader (pytorch specific)\n",
    "    def train(self, train_loader, nb_epochs=10, lr=0.0001, verbose=True):\n",
    "        for epoch in range(nb_epochs):\n",
    "            losses, error = [], []\n",
    "            for _, data in enumerate(train_loader):\n",
    "        \n",
    "                x0, y0 = data\n",
    "                self.grads_zero_()      \n",
    "        \n",
    "                # Forward Pass\n",
    "                out = self.forward(x0)\n",
    "        \n",
    "                if verbose: \n",
    "                    losses.append(self.loss(out, y0))\n",
    "                    _, t = torch.max(y0, dim=1)\n",
    "                    _, x = torch.max(out, dim=1)\n",
    "                    error.append(torch.mean((t != x).float()))\n",
    "        \n",
    "                # Backward Pass\n",
    "                self.backward(y0)\n",
    "                # Optimization step\n",
    "                self.optimize_step(lr)\n",
    "        \n",
    "            if verbose:\n",
    "                trloss, trerr = sum(losses)/(len(losses)), sum(error)/len(error)\n",
    "                print(\"-\" * 60)\n",
    "                print(f\"Epoch : {epoch+1}: \")\n",
    "                print(f\"training loss : {trloss:.5f}, training error : {trerr*100:.3f}%\")\n",
    "\n",
    "    # Function to evaulate model performance on a test set\n",
    "    def evaluate_test(self, test_loader):\n",
    "        losses, error = [], []\n",
    "    \n",
    "        for data in test_loader:\n",
    "            inputs, targets = data\n",
    "            out = self.forward(inputs)\n",
    "            losses.append(self.loss(out, targets))\n",
    "            _, t = torch.max(targets, dim=1)\n",
    "            _, x = torch.max(out, dim=1)\n",
    "            error.append(torch.mean((t != x).float()))\n",
    "    \n",
    "        return sum(losses)/(len(losses)), sum(error)/len(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqdnN4wa2u1p"
   },
   "source": [
    "## Importing and Loading data\n",
    "\n",
    "Although ideally I would keep this code in a separate module, I have included it here for the sake of completion.\n",
    "\n",
    "Note: Instead of Input and Target tensors, I am using **Dataset Loaders** (pytorch specific) which generate mini-batches that will be used by the model defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KkNfyLWAPQzR"
   },
   "outputs": [],
   "source": [
    "# Function to perform one-hot encoding of target values\n",
    "def convert_to_one_hot_labels(input, target):\n",
    "    tmp = input.new(target.size(0), target.max() + 1).fill_(-1)\n",
    "    for k in range(0, target.size(0)):\n",
    "        tmp[k, target[k]] = 1\n",
    "    return tmp\n",
    "\n",
    "# Function to load the mnist data and return mini-batch loaders to be used by the model\n",
    "def load_mnist(one_hot_labels = False, normalize = False, flatten = True, batch_size=4):\n",
    "\n",
    "    data_dir = os.environ.get('PYTORCH_DATA_DIR')\n",
    "    if data_dir is None:\n",
    "        data_dir = './data'\n",
    "\n",
    "    mnist_train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "    mnist_test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "\n",
    "    train_input = mnist_train_set.train_data.view(-1, 1, 28, 28).float()\n",
    "    train_target = mnist_train_set.train_labels\n",
    "    test_input = mnist_test_set.test_data.view(-1, 1, 28, 28).float()\n",
    "    test_target = mnist_test_set.test_labels\n",
    "\n",
    "    if flatten:\n",
    "        train_input = train_input.clone().view(train_input.size(0), -1)\n",
    "        test_input = test_input.clone().view(test_input.size(0), -1)\n",
    "\n",
    "    train_input = train_input.narrow(0, 0, 1000)\n",
    "    train_target = train_target.narrow(0, 0, 1000)\n",
    "    test_input = test_input.narrow(0, 0, 1000)\n",
    "    test_target = test_target.narrow(0, 0, 1000)\n",
    "\n",
    "    if one_hot_labels:\n",
    "        train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "        test_target = convert_to_one_hot_labels(test_input, test_target)\n",
    "\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    train_target *= 0.9\n",
    "    test_target *= 0.9\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(train_input, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_input, test_target)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, train_input.size(1), train_target.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dPcNnwGhPdL3"
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader, input_dim, output_dim = load_mnist (one_hot_labels=True, normalize=True, flatten=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oiomyful22tC"
   },
   "source": [
    "## Training the MLP model\n",
    "\n",
    "Train the model using 100 epochs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 26067,
     "output_extras": [
      {
       "item_id": 22
      },
      {
       "item_id": 45
      },
      {
       "item_id": 67
      },
      {
       "item_id": 89
      },
      {
       "item_id": 111
      },
      {
       "item_id": 133
      },
      {
       "item_id": 155
      },
      {
       "item_id": 167
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45387,
     "status": "ok",
     "timestamp": 1519072497386,
     "user": {
      "displayName": "Sangeet Kar",
      "photoUrl": "//lh4.googleusercontent.com/-7ebPpKsvs9M/AAAAAAAAAAI/AAAAAAAAA0M/Aswm5Tnukds/s50-c-k-no/photo.jpg",
      "userId": "115913212643122775136"
     },
     "user_tz": -60
    },
    "id": "5MAJq3cxPmLA",
    "outputId": "05a79b96-6caf-4a1d-91db-6811a47cad66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 1: \n",
      "training loss : 0.71964, training error : 89.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 2: \n",
      "training loss : 0.55784, training error : 89.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 3: \n",
      "training loss : 0.32199, training error : 90.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 4: \n",
      "training loss : 0.29493, training error : 88.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 5: \n",
      "training loss : 0.29380, training error : 89.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 6: \n",
      "training loss : 0.29350, training error : 88.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 7: \n",
      "training loss : 0.29335, training error : 88.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 8: \n",
      "training loss : 0.29319, training error : 88.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 9: \n",
      "training loss : 0.29302, training error : 88.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 10: \n",
      "training loss : 0.29289, training error : 88.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 11: \n",
      "training loss : 0.29279, training error : 89.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 12: \n",
      "training loss : 0.29264, training error : 88.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 13: \n",
      "training loss : 0.29250, training error : 87.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 14: \n",
      "training loss : 0.29231, training error : 87.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 15: \n",
      "training loss : 0.29201, training error : 86.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 16: \n",
      "training loss : 0.29157, training error : 85.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 17: \n",
      "training loss : 0.29082, training error : 86.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 18: \n",
      "training loss : 0.28951, training error : 81.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 19: \n",
      "training loss : 0.28728, training error : 78.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 20: \n",
      "training loss : 0.28362, training error : 77.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 21: \n",
      "training loss : 0.27878, training error : 78.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 22: \n",
      "training loss : 0.27385, training error : 78.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 23: \n",
      "training loss : 0.26972, training error : 77.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 24: \n",
      "training loss : 0.26641, training error : 76.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 25: \n",
      "training loss : 0.26369, training error : 76.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 26: \n",
      "training loss : 0.26121, training error : 75.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 27: \n",
      "training loss : 0.25849, training error : 72.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 28: \n",
      "training loss : 0.25552, training error : 70.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 29: \n",
      "training loss : 0.25224, training error : 69.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 30: \n",
      "training loss : 0.24867, training error : 69.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 31: \n",
      "training loss : 0.24476, training error : 68.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 32: \n",
      "training loss : 0.24040, training error : 67.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 33: \n",
      "training loss : 0.23551, training error : 65.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 34: \n",
      "training loss : 0.23021, training error : 63.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 35: \n",
      "training loss : 0.22461, training error : 59.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 36: \n",
      "training loss : 0.21891, training error : 56.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 37: \n",
      "training loss : 0.21320, training error : 52.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 38: \n",
      "training loss : 0.20752, training error : 50.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 39: \n",
      "training loss : 0.20186, training error : 48.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 40: \n",
      "training loss : 0.19620, training error : 45.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 41: \n",
      "training loss : 0.19062, training error : 43.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 42: \n",
      "training loss : 0.18511, training error : 40.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 43: \n",
      "training loss : 0.17974, training error : 38.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 44: \n",
      "training loss : 0.17447, training error : 35.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 45: \n",
      "training loss : 0.16938, training error : 33.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 46: \n",
      "training loss : 0.16443, training error : 32.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 47: \n",
      "training loss : 0.15953, training error : 31.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 48: \n",
      "training loss : 0.15494, training error : 28.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 49: \n",
      "training loss : 0.15027, training error : 28.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 50: \n",
      "training loss : 0.14587, training error : 27.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 51: \n",
      "training loss : 0.14166, training error : 27.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 52: \n",
      "training loss : 0.13754, training error : 25.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 53: \n",
      "training loss : 0.13359, training error : 24.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 54: \n",
      "training loss : 0.12986, training error : 24.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 55: \n",
      "training loss : 0.12616, training error : 21.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 56: \n",
      "training loss : 0.12277, training error : 22.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 57: \n",
      "training loss : 0.11932, training error : 19.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 58: \n",
      "training loss : 0.11599, training error : 18.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 59: \n",
      "training loss : 0.11268, training error : 18.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 60: \n",
      "training loss : 0.10942, training error : 16.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 61: \n",
      "training loss : 0.10616, training error : 15.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 62: \n",
      "training loss : 0.10292, training error : 14.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 63: \n",
      "training loss : 0.09966, training error : 13.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 64: \n",
      "training loss : 0.09646, training error : 12.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 65: \n",
      "training loss : 0.09324, training error : 10.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 66: \n",
      "training loss : 0.09015, training error : 10.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 67: \n",
      "training loss : 0.08716, training error : 10.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 68: \n",
      "training loss : 0.08418, training error : 9.700%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 69: \n",
      "training loss : 0.08135, training error : 9.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 70: \n",
      "training loss : 0.07858, training error : 9.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 71: \n",
      "training loss : 0.07614, training error : 9.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 72: \n",
      "training loss : 0.07370, training error : 8.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 73: \n",
      "training loss : 0.07149, training error : 7.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 74: \n",
      "training loss : 0.06928, training error : 7.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 75: \n",
      "training loss : 0.06730, training error : 7.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 76: \n",
      "training loss : 0.06540, training error : 7.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 77: \n",
      "training loss : 0.06356, training error : 7.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 78: \n",
      "training loss : 0.06196, training error : 6.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 79: \n",
      "training loss : 0.06029, training error : 7.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 80: \n",
      "training loss : 0.05883, training error : 6.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 81: \n",
      "training loss : 0.05734, training error : 6.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 82: \n",
      "training loss : 0.05597, training error : 6.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 83: \n",
      "training loss : 0.05466, training error : 6.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 84: \n",
      "training loss : 0.05348, training error : 5.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 85: \n",
      "training loss : 0.05233, training error : 5.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 86: \n",
      "training loss : 0.05122, training error : 5.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 87: \n",
      "training loss : 0.05011, training error : 5.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 88: \n",
      "training loss : 0.04911, training error : 5.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 89: \n",
      "training loss : 0.04809, training error : 4.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 90: \n",
      "training loss : 0.04715, training error : 5.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 91: \n",
      "training loss : 0.04617, training error : 5.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 92: \n",
      "training loss : 0.04538, training error : 4.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 93: \n",
      "training loss : 0.04450, training error : 4.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 94: \n",
      "training loss : 0.04369, training error : 4.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 95: \n",
      "training loss : 0.04283, training error : 4.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 96: \n",
      "training loss : 0.04215, training error : 4.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 97: \n",
      "training loss : 0.04141, training error : 4.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 98: \n",
      "training loss : 0.04069, training error : 4.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 99: \n",
      "training loss : 0.03997, training error : 4.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 100: \n",
      "training loss : 0.03932, training error : 4.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 101: \n",
      "training loss : 0.03864, training error : 4.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 102: \n",
      "training loss : 0.03797, training error : 3.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 103: \n",
      "training loss : 0.03736, training error : 3.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 104: \n",
      "training loss : 0.03677, training error : 3.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 105: \n",
      "training loss : 0.03622, training error : 3.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 106: \n",
      "training loss : 0.03562, training error : 3.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 107: \n",
      "training loss : 0.03504, training error : 3.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 108: \n",
      "training loss : 0.03452, training error : 3.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 109: \n",
      "training loss : 0.03400, training error : 3.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 110: \n",
      "training loss : 0.03343, training error : 3.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 111: \n",
      "training loss : 0.03297, training error : 3.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 112: \n",
      "training loss : 0.03255, training error : 2.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 113: \n",
      "training loss : 0.03203, training error : 2.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 114: \n",
      "training loss : 0.03154, training error : 2.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 115: \n",
      "training loss : 0.03116, training error : 2.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 116: \n",
      "training loss : 0.03065, training error : 2.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 117: \n",
      "training loss : 0.03021, training error : 2.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 118: \n",
      "training loss : 0.02984, training error : 2.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 119: \n",
      "training loss : 0.02942, training error : 2.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 120: \n",
      "training loss : 0.02902, training error : 2.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 121: \n",
      "training loss : 0.02861, training error : 2.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 122: \n",
      "training loss : 0.02827, training error : 2.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 123: \n",
      "training loss : 0.02786, training error : 2.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 124: \n",
      "training loss : 0.02753, training error : 2.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 125: \n",
      "training loss : 0.02714, training error : 2.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 126: \n",
      "training loss : 0.02676, training error : 2.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 127: \n",
      "training loss : 0.02640, training error : 2.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 128: \n",
      "training loss : 0.02612, training error : 2.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 129: \n",
      "training loss : 0.02577, training error : 2.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 130: \n",
      "training loss : 0.02548, training error : 2.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 131: \n",
      "training loss : 0.02514, training error : 2.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 132: \n",
      "training loss : 0.02485, training error : 2.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 133: \n",
      "training loss : 0.02456, training error : 2.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 134: \n",
      "training loss : 0.02427, training error : 2.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 135: \n",
      "training loss : 0.02400, training error : 1.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 136: \n",
      "training loss : 0.02373, training error : 2.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 137: \n",
      "training loss : 0.02344, training error : 1.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 138: \n",
      "training loss : 0.02318, training error : 1.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 139: \n",
      "training loss : 0.02293, training error : 1.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 140: \n",
      "training loss : 0.02267, training error : 1.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 141: \n",
      "training loss : 0.02244, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 142: \n",
      "training loss : 0.02219, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 143: \n",
      "training loss : 0.02195, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 144: \n",
      "training loss : 0.02174, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 145: \n",
      "training loss : 0.02149, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 146: \n",
      "training loss : 0.02128, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 147: \n",
      "training loss : 0.02106, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 148: \n",
      "training loss : 0.02085, training error : 1.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 149: \n",
      "training loss : 0.02065, training error : 1.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 150: \n",
      "training loss : 0.02043, training error : 1.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 151: \n",
      "training loss : 0.02023, training error : 1.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 152: \n",
      "training loss : 0.02005, training error : 1.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 153: \n",
      "training loss : 0.01980, training error : 1.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 154: \n",
      "training loss : 0.01968, training error : 1.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 155: \n",
      "training loss : 0.01946, training error : 1.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 156: \n",
      "training loss : 0.01933, training error : 1.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 157: \n",
      "training loss : 0.01913, training error : 1.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 158: \n",
      "training loss : 0.01896, training error : 1.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 159: \n",
      "training loss : 0.01880, training error : 1.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 160: \n",
      "training loss : 0.01864, training error : 1.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 161: \n",
      "training loss : 0.01847, training error : 1.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 162: \n",
      "training loss : 0.01830, training error : 1.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 163: \n",
      "training loss : 0.01818, training error : 1.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 164: \n",
      "training loss : 0.01801, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 165: \n",
      "training loss : 0.01782, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 166: \n",
      "training loss : 0.01772, training error : 1.200%\n",
      "------------------------------------------------------------\n",
      "Epoch : 167: \n",
      "training loss : 0.01757, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 168: \n",
      "training loss : 0.01744, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 169: \n",
      "training loss : 0.01731, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 170: \n",
      "training loss : 0.01716, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 171: \n",
      "training loss : 0.01702, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 172: \n",
      "training loss : 0.01689, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 173: \n",
      "training loss : 0.01676, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 174: \n",
      "training loss : 0.01664, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 175: \n",
      "training loss : 0.01652, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 176: \n",
      "training loss : 0.01639, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 177: \n",
      "training loss : 0.01628, training error : 1.100%\n",
      "------------------------------------------------------------\n",
      "Epoch : 178: \n",
      "training loss : 0.01616, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 179: \n",
      "training loss : 0.01605, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 180: \n",
      "training loss : 0.01594, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 181: \n",
      "training loss : 0.01583, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 182: \n",
      "training loss : 0.01570, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 183: \n",
      "training loss : 0.01562, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 184: \n",
      "training loss : 0.01551, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 185: \n",
      "training loss : 0.01541, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 186: \n",
      "training loss : 0.01530, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 187: \n",
      "training loss : 0.01520, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 188: \n",
      "training loss : 0.01508, training error : 1.000%\n",
      "------------------------------------------------------------\n",
      "Epoch : 189: \n",
      "training loss : 0.01501, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 190: \n",
      "training loss : 0.01491, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 191: \n",
      "training loss : 0.01482, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 192: \n",
      "training loss : 0.01473, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 193: \n",
      "training loss : 0.01463, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 194: \n",
      "training loss : 0.01455, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 195: \n",
      "training loss : 0.01446, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 196: \n",
      "training loss : 0.01438, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 197: \n",
      "training loss : 0.01430, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 198: \n",
      "training loss : 0.01421, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 199: \n",
      "training loss : 0.01413, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 200: \n",
      "training loss : 0.01405, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 201: \n",
      "training loss : 0.01398, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 202: \n",
      "training loss : 0.01390, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 203: \n",
      "training loss : 0.01383, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 204: \n",
      "training loss : 0.01375, training error : 0.900%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 205: \n",
      "training loss : 0.01367, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 206: \n",
      "training loss : 0.01361, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 207: \n",
      "training loss : 0.01354, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 208: \n",
      "training loss : 0.01347, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 209: \n",
      "training loss : 0.01339, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 210: \n",
      "training loss : 0.01333, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 211: \n",
      "training loss : 0.01326, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 212: \n",
      "training loss : 0.01319, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 213: \n",
      "training loss : 0.01313, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 214: \n",
      "training loss : 0.01307, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 215: \n",
      "training loss : 0.01301, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 216: \n",
      "training loss : 0.01295, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 217: \n",
      "training loss : 0.01289, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 218: \n",
      "training loss : 0.01283, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 219: \n",
      "training loss : 0.01277, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 220: \n",
      "training loss : 0.01271, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 221: \n",
      "training loss : 0.01266, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 222: \n",
      "training loss : 0.01260, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 223: \n",
      "training loss : 0.01255, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 224: \n",
      "training loss : 0.01249, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 225: \n",
      "training loss : 0.01244, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 226: \n",
      "training loss : 0.01237, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 227: \n",
      "training loss : 0.01234, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 228: \n",
      "training loss : 0.01229, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 229: \n",
      "training loss : 0.01223, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 230: \n",
      "training loss : 0.01218, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 231: \n",
      "training loss : 0.01213, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 232: \n",
      "training loss : 0.01208, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 233: \n",
      "training loss : 0.01204, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 234: \n",
      "training loss : 0.01199, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 235: \n",
      "training loss : 0.01194, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 236: \n",
      "training loss : 0.01189, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 237: \n",
      "training loss : 0.01185, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 238: \n",
      "training loss : 0.01180, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 239: \n",
      "training loss : 0.01176, training error : 0.900%\n",
      "------------------------------------------------------------\n",
      "Epoch : 240: \n",
      "training loss : 0.01171, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 241: \n",
      "training loss : 0.01166, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 242: \n",
      "training loss : 0.01162, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 243: \n",
      "training loss : 0.01157, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 244: \n",
      "training loss : 0.01153, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 245: \n",
      "training loss : 0.01149, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 246: \n",
      "training loss : 0.01145, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 247: \n",
      "training loss : 0.01141, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 248: \n",
      "training loss : 0.01136, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 249: \n",
      "training loss : 0.01132, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 250: \n",
      "training loss : 0.01128, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 251: \n",
      "training loss : 0.01123, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 252: \n",
      "training loss : 0.01120, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 253: \n",
      "training loss : 0.01116, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 254: \n",
      "training loss : 0.01111, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 255: \n",
      "training loss : 0.01108, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 256: \n",
      "training loss : 0.01104, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 257: \n",
      "training loss : 0.01100, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 258: \n",
      "training loss : 0.01096, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 259: \n",
      "training loss : 0.01092, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 260: \n",
      "training loss : 0.01089, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 261: \n",
      "training loss : 0.01085, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 262: \n",
      "training loss : 0.01082, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 263: \n",
      "training loss : 0.01078, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 264: \n",
      "training loss : 0.01075, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 265: \n",
      "training loss : 0.01071, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 266: \n",
      "training loss : 0.01068, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 267: \n",
      "training loss : 0.01065, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 268: \n",
      "training loss : 0.01061, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 269: \n",
      "training loss : 0.01058, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 270: \n",
      "training loss : 0.01055, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 271: \n",
      "training loss : 0.01052, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 272: \n",
      "training loss : 0.01049, training error : 0.800%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 273: \n",
      "training loss : 0.01046, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 274: \n",
      "training loss : 0.01043, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 275: \n",
      "training loss : 0.01040, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 276: \n",
      "training loss : 0.01037, training error : 0.800%\n",
      "------------------------------------------------------------\n",
      "Epoch : 277: \n",
      "training loss : 0.01034, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 278: \n",
      "training loss : 0.01031, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 279: \n",
      "training loss : 0.01028, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 280: \n",
      "training loss : 0.01025, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 281: \n",
      "training loss : 0.01022, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 282: \n",
      "training loss : 0.01020, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 283: \n",
      "training loss : 0.01017, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 284: \n",
      "training loss : 0.01014, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 285: \n",
      "training loss : 0.01011, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 286: \n",
      "training loss : 0.01009, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 287: \n",
      "training loss : 0.01006, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 288: \n",
      "training loss : 0.01003, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 289: \n",
      "training loss : 0.01000, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 290: \n",
      "training loss : 0.00998, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 291: \n",
      "training loss : 0.00995, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 292: \n",
      "training loss : 0.00992, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 293: \n",
      "training loss : 0.00990, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 294: \n",
      "training loss : 0.00987, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 295: \n",
      "training loss : 0.00984, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 296: \n",
      "training loss : 0.00982, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 297: \n",
      "training loss : 0.00980, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 298: \n",
      "training loss : 0.00977, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 299: \n",
      "training loss : 0.00974, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 300: \n",
      "training loss : 0.00972, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 301: \n",
      "training loss : 0.00970, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 302: \n",
      "training loss : 0.00968, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 303: \n",
      "training loss : 0.00965, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 304: \n",
      "training loss : 0.00963, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 305: \n",
      "training loss : 0.00961, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 306: \n",
      "training loss : 0.00958, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 307: \n",
      "training loss : 0.00956, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 308: \n",
      "training loss : 0.00954, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 309: \n",
      "training loss : 0.00952, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 310: \n",
      "training loss : 0.00950, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 311: \n",
      "training loss : 0.00948, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 312: \n",
      "training loss : 0.00945, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 313: \n",
      "training loss : 0.00943, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 314: \n",
      "training loss : 0.00941, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 315: \n",
      "training loss : 0.00939, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 316: \n",
      "training loss : 0.00937, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 317: \n",
      "training loss : 0.00936, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 318: \n",
      "training loss : 0.00933, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 319: \n",
      "training loss : 0.00931, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 320: \n",
      "training loss : 0.00929, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 321: \n",
      "training loss : 0.00927, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 322: \n",
      "training loss : 0.00926, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 323: \n",
      "training loss : 0.00924, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 324: \n",
      "training loss : 0.00922, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 325: \n",
      "training loss : 0.00920, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 326: \n",
      "training loss : 0.00919, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 327: \n",
      "training loss : 0.00917, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 328: \n",
      "training loss : 0.00915, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 329: \n",
      "training loss : 0.00913, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 330: \n",
      "training loss : 0.00911, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 331: \n",
      "training loss : 0.00910, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 332: \n",
      "training loss : 0.00908, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 333: \n",
      "training loss : 0.00906, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 334: \n",
      "training loss : 0.00905, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 335: \n",
      "training loss : 0.00903, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 336: \n",
      "training loss : 0.00901, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 337: \n",
      "training loss : 0.00900, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 338: \n",
      "training loss : 0.00898, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 339: \n",
      "training loss : 0.00897, training error : 0.700%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 340: \n",
      "training loss : 0.00895, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 341: \n",
      "training loss : 0.00893, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 342: \n",
      "training loss : 0.00892, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 343: \n",
      "training loss : 0.00890, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 344: \n",
      "training loss : 0.00889, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 345: \n",
      "training loss : 0.00887, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 346: \n",
      "training loss : 0.00885, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 347: \n",
      "training loss : 0.00884, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 348: \n",
      "training loss : 0.00883, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 349: \n",
      "training loss : 0.00881, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 350: \n",
      "training loss : 0.00880, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 351: \n",
      "training loss : 0.00878, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 352: \n",
      "training loss : 0.00877, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 353: \n",
      "training loss : 0.00875, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 354: \n",
      "training loss : 0.00874, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 355: \n",
      "training loss : 0.00872, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 356: \n",
      "training loss : 0.00871, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 357: \n",
      "training loss : 0.00869, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 358: \n",
      "training loss : 0.00868, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 359: \n",
      "training loss : 0.00867, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 360: \n",
      "training loss : 0.00865, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 361: \n",
      "training loss : 0.00864, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 362: \n",
      "training loss : 0.00862, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 363: \n",
      "training loss : 0.00861, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 364: \n",
      "training loss : 0.00859, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 365: \n",
      "training loss : 0.00858, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 366: \n",
      "training loss : 0.00856, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 367: \n",
      "training loss : 0.00855, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 368: \n",
      "training loss : 0.00854, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 369: \n",
      "training loss : 0.00852, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 370: \n",
      "training loss : 0.00851, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 371: \n",
      "training loss : 0.00850, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 372: \n",
      "training loss : 0.00848, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 373: \n",
      "training loss : 0.00847, training error : 0.700%\n",
      "------------------------------------------------------------\n",
      "Epoch : 374: \n",
      "training loss : 0.00845, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 375: \n",
      "training loss : 0.00844, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 376: \n",
      "training loss : 0.00843, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 377: \n",
      "training loss : 0.00841, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 378: \n",
      "training loss : 0.00840, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 379: \n",
      "training loss : 0.00838, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 380: \n",
      "training loss : 0.00837, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 381: \n",
      "training loss : 0.00836, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 382: \n",
      "training loss : 0.00834, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 383: \n",
      "training loss : 0.00833, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 384: \n",
      "training loss : 0.00832, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 385: \n",
      "training loss : 0.00830, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 386: \n",
      "training loss : 0.00829, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 387: \n",
      "training loss : 0.00827, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 388: \n",
      "training loss : 0.00826, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 389: \n",
      "training loss : 0.00825, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 390: \n",
      "training loss : 0.00823, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 391: \n",
      "training loss : 0.00822, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 392: \n",
      "training loss : 0.00820, training error : 0.600%\n",
      "------------------------------------------------------------\n",
      "Epoch : 393: \n",
      "training loss : 0.00819, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 394: \n",
      "training loss : 0.00818, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 395: \n",
      "training loss : 0.00816, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 396: \n",
      "training loss : 0.00815, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 397: \n",
      "training loss : 0.00813, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 398: \n",
      "training loss : 0.00812, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 399: \n",
      "training loss : 0.00810, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 400: \n",
      "training loss : 0.00808, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 401: \n",
      "training loss : 0.00807, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 402: \n",
      "training loss : 0.00805, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 403: \n",
      "training loss : 0.00804, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 404: \n",
      "training loss : 0.00802, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 405: \n",
      "training loss : 0.00800, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 406: \n",
      "training loss : 0.00799, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 407: \n",
      "training loss : 0.00797, training error : 0.500%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 408: \n",
      "training loss : 0.00795, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 409: \n",
      "training loss : 0.00794, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 410: \n",
      "training loss : 0.00792, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 411: \n",
      "training loss : 0.00790, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 412: \n",
      "training loss : 0.00788, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 413: \n",
      "training loss : 0.00787, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 414: \n",
      "training loss : 0.00785, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 415: \n",
      "training loss : 0.00783, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 416: \n",
      "training loss : 0.00782, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 417: \n",
      "training loss : 0.00780, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 418: \n",
      "training loss : 0.00778, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 419: \n",
      "training loss : 0.00777, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 420: \n",
      "training loss : 0.00775, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 421: \n",
      "training loss : 0.00773, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 422: \n",
      "training loss : 0.00772, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 423: \n",
      "training loss : 0.00770, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 424: \n",
      "training loss : 0.00769, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 425: \n",
      "training loss : 0.00767, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 426: \n",
      "training loss : 0.00766, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 427: \n",
      "training loss : 0.00764, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 428: \n",
      "training loss : 0.00763, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 429: \n",
      "training loss : 0.00761, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 430: \n",
      "training loss : 0.00760, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 431: \n",
      "training loss : 0.00759, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 432: \n",
      "training loss : 0.00757, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 433: \n",
      "training loss : 0.00756, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 434: \n",
      "training loss : 0.00754, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 435: \n",
      "training loss : 0.00753, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 436: \n",
      "training loss : 0.00752, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 437: \n",
      "training loss : 0.00750, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 438: \n",
      "training loss : 0.00749, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 439: \n",
      "training loss : 0.00748, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 440: \n",
      "training loss : 0.00747, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 441: \n",
      "training loss : 0.00745, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 442: \n",
      "training loss : 0.00744, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 443: \n",
      "training loss : 0.00743, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 444: \n",
      "training loss : 0.00742, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 445: \n",
      "training loss : 0.00740, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 446: \n",
      "training loss : 0.00739, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 447: \n",
      "training loss : 0.00738, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 448: \n",
      "training loss : 0.00736, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 449: \n",
      "training loss : 0.00735, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 450: \n",
      "training loss : 0.00734, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 451: \n",
      "training loss : 0.00733, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 452: \n",
      "training loss : 0.00731, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 453: \n",
      "training loss : 0.00730, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 454: \n",
      "training loss : 0.00729, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 455: \n",
      "training loss : 0.00728, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 456: \n",
      "training loss : 0.00726, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 457: \n",
      "training loss : 0.00725, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 458: \n",
      "training loss : 0.00724, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 459: \n",
      "training loss : 0.00723, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 460: \n",
      "training loss : 0.00721, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 461: \n",
      "training loss : 0.00720, training error : 0.500%\n",
      "------------------------------------------------------------\n",
      "Epoch : 462: \n",
      "training loss : 0.00719, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 463: \n",
      "training loss : 0.00717, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 464: \n",
      "training loss : 0.00716, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 465: \n",
      "training loss : 0.00715, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 466: \n",
      "training loss : 0.00713, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 467: \n",
      "training loss : 0.00712, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 468: \n",
      "training loss : 0.00711, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 469: \n",
      "training loss : 0.00709, training error : 0.400%\n",
      "------------------------------------------------------------\n",
      "Epoch : 470: \n",
      "training loss : 0.00708, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 471: \n",
      "training loss : 0.00706, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 472: \n",
      "training loss : 0.00705, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 473: \n",
      "training loss : 0.00704, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 474: \n",
      "training loss : 0.00702, training error : 0.300%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch : 475: \n",
      "training loss : 0.00701, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 476: \n",
      "training loss : 0.00699, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 477: \n",
      "training loss : 0.00698, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 478: \n",
      "training loss : 0.00696, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 479: \n",
      "training loss : 0.00695, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 480: \n",
      "training loss : 0.00693, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 481: \n",
      "training loss : 0.00692, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 482: \n",
      "training loss : 0.00690, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 483: \n",
      "training loss : 0.00689, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 484: \n",
      "training loss : 0.00687, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 485: \n",
      "training loss : 0.00686, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 486: \n",
      "training loss : 0.00685, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 487: \n",
      "training loss : 0.00683, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 488: \n",
      "training loss : 0.00682, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 489: \n",
      "training loss : 0.00681, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 490: \n",
      "training loss : 0.00679, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 491: \n",
      "training loss : 0.00678, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 492: \n",
      "training loss : 0.00677, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 493: \n",
      "training loss : 0.00675, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 494: \n",
      "training loss : 0.00674, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 495: \n",
      "training loss : 0.00673, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 496: \n",
      "training loss : 0.00671, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 497: \n",
      "training loss : 0.00670, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 498: \n",
      "training loss : 0.00669, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 499: \n",
      "training loss : 0.00668, training error : 0.300%\n",
      "------------------------------------------------------------\n",
      "Epoch : 500: \n",
      "training loss : 0.00667, training error : 0.300%\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(input_dim, output_dim, hidden_dim=50)\n",
    "model.train(train_loader, nb_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLVZvbmc29QR"
   },
   "source": [
    "## Evaluating the model with a test set\n",
    "\n",
    "Evaluating the performance of the model using the given test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1519072498012,
     "user": {
      "displayName": "Sangeet Kar",
      "photoUrl": "//lh4.googleusercontent.com/-7ebPpKsvs9M/AAAAAAAAAAI/AAAAAAAAA0M/Aswm5Tnukds/s50-c-k-no/photo.jpg",
      "userId": "115913212643122775136"
     },
     "user_tz": -60
    },
    "id": "tqIhzwuyQAdf",
    "outputId": "bc0201eb-949c-4e6b-ea7b-f01b803c2867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error rate : 14.400%\n"
     ]
    }
   ],
   "source": [
    "l, e = model.evaluate_test(test_loader)\n",
    "print(f\"Test Error rate : {e*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Hdit-tH0DG2"
   },
   "source": [
    "## Visualizing Sample Images and inferred labels\n",
    "\n",
    "Now let's use the MLP model to determine labels of some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1519073235731,
     "user": {
      "displayName": "Sangeet Kar",
      "photoUrl": "//lh4.googleusercontent.com/-7ebPpKsvs9M/AAAAAAAAAAI/AAAAAAAAA0M/Aswm5Tnukds/s50-c-k-no/photo.jpg",
      "userId": "115913212643122775136"
     },
     "user_tz": -60
    },
    "id": "qKnpO-PStKA9",
    "outputId": "6e208fe7-c8e5-4a65-be0b-ad03c05a0c67"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac9cdb2470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_images, _ = next(iter(test_loader))\n",
    "labels = model.forward(test_images)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4)\n",
    "for i,ax in enumerate(axes):\n",
    "    _, label = torch.max(labels[i], dim=0)\n",
    "    ax.imshow(test_images[i].view(28,28).numpy(), interpolation='nearest')\n",
    "    ax.set_title(f\"Label : {label[0]}\")\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5-j3uo03SD8"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "I really enjoyed this learning exercise.\n",
    "\n",
    "My next steps will be to:\n",
    "\n",
    "* Generalize the number of hidden layers\n",
    "* Use regularization\n",
    "* Include Btachnormalization\n",
    "* Implement optimization algorithms like Adam etc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MLP from scratch.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
